Project Scan Report
Root Directory: C:\Users\pc\Desktop\shahmir
Platform: win32
Custom skip directories: PKRV, PKISRV, PKFRV
==================================================


### File: main.py ###
# main.py
from fastapi import FastAPI, HTTPException
from fastapi.responses import HTMLResponse
from typing import List, Dict, Any
import pandas as pd
import os
import uvicorn # Required to run the FastAPI app

# --- Configuration ---
# Define the base directory where your report folders are located
BASE_DATA_DIR = "." # Current directory, adjust if your folders are elsewhere

REPORT_FOLDERS = {
    "PKISRV": os.path.join(BASE_DATA_DIR, "PKISRV"),
    "PKRV": os.path.join(BASE_DATA_DIR, "PKRV"),
    "PKFRV": os.path.join(BASE_DATA_DIR, "PKFRV")
}

app = FastAPI(
    title="MUFAP Reports Data Viewer",
    description="API to view and access MUFAP CSV report data.",
    version="1.0.0"
)

# --- Helper Function for CSV Reading ---
def read_csv_with_flexible_encoding(filepath: str) -> pd.DataFrame:
    """
    Reads a CSV file, attempting multiple common encodings.
    Skips initial rows that might contain metadata before the actual header.
    """
    encodings_to_try = ['utf-8', 'latin1', 'cp1252']
    
    # Heuristic: Try to find the actual header row if it's not the first row.
    # This is a common issue with reports that have titles/metadata at the top.
    # We'll try reading with different skiprows values.
    for skip_rows in range(5): # Try skipping 0 to 4 rows
        for encoding in encodings_to_try:
            try:
                # Attempt to read, assuming the first non-empty row after skipping is the header
                df = pd.read_csv(filepath, encoding=encoding, skiprows=skip_rows)
                
                # Check if the header looks like actual column names (not just a single long string)
                # If the first column name is a long string or looks like metadata, try skipping more
                if df.empty or len(df.columns) < 2 or (len(df.columns) == 1 and "Unnamed" in df.columns[0]):
                    continue # This header might still be part of metadata, try skipping more
                
                # Drop columns that are entirely unnamed (e.g., 'Unnamed: 0', 'Unnamed: 1')
                # These often result from irregular CSV formatting
                df = df.loc[:, ~df.columns.str.contains('^Unnamed', na=False)]
                
                # Drop rows where all values are NaN (empty rows)
                df.dropna(how='all', inplace=True)

                # Reset index after dropping rows
                df.reset_index(drop=True, inplace=True)

                # If the first column is still problematic, consider it as a sign of bad header detection
                # and continue to the next skip_rows attempt.
                # This is a heuristic and might need fine-tuning for specific files.
                if not df.empty and df.columns[0].strip() == "":
                    continue

                return df
            except pd.errors.EmptyDataError:
                # File is empty or no data after skipping rows
                continue
            except UnicodeDecodeError:
                # Encoding failed, try next encoding
                continue
            except Exception as e:
                # Other pandas reading errors, print and try next encoding/skip_rows
                print(f"Error reading {filepath} with encoding {encoding} and skiprows {skip_rows}: {e}")
                continue
    
    raise ValueError(f"Could not read CSV file: {filepath} with any of the tried encodings/skiprows.")


# --- API Endpoints ---

@app.get("/", response_class=HTMLResponse, summary="Home Page")
async def read_root():
    """
    Provides a simple HTML home page with links to available report types.
    """
    html_content = """
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>MUFAP Reports Viewer</title>
        <script src="https://cdn.tailwindcss.com"></script>
        <style>
            body { font-family: 'Inter', sans-serif; }
        </style>
    </head>
    <body class="bg-gray-100 min-h-screen flex flex-col items-center justify-center p-4">
        <div class="bg-white p-8 rounded-lg shadow-lg max-w-md w-full text-center">
            <h1 class="text-3xl font-bold text-gray-800 mb-6">Welcome to MUFAP Reports Viewer</h1>
            <p class="text-gray-600 mb-8">Select a report type to view available files:</p>
            <div class="space-y-4">
                <a href="/reports/PKISRV" class="block bg-blue-500 hover:bg-blue-600 text-white font-semibold py-3 px-6 rounded-lg shadow-md transition duration-300 ease-in-out transform hover:scale-105">
                    View PKISRV Reports
                </a>
                <a href="/reports/PKRV" class="block bg-green-500 hover:bg-green-600 text-white font-semibold py-3 px-6 rounded-lg shadow-md transition duration-300 ease-in-out transform hover:scale-105">
                    View PKRV Reports
                </a>
                <a href="/reports/PKFRV" class="block bg-purple-500 hover:bg-purple-600 text-white font-semibold py-3 px-6 rounded-lg shadow-md transition duration-300 ease-in-out transform hover:scale-105">
                    View PKFRV Reports
                </a>
            </div>
            <p class="text-sm text-gray-500 mt-8">Note: Data consistency varies across files. Pre-processing is recommended for structured analysis.</p>
        </div>
    </body>
    </html>
    """
    return HTMLResponse(content=html_content)

@app.get("/reports/{report_type}", response_model=List[str], summary="List Files by Report Type")
async def list_files(report_type: str):
    """
    Lists all available CSV file names for a given report type.
    """
    if report_type not in REPORT_FOLDERS:
        raise HTTPException(status_code=404, detail="Report type not found.")
    
    folder_path = REPORT_FOLDERS[report_type]
    if not os.path.exists(folder_path) or not os.path.isdir(folder_path):
        raise HTTPException(status_code=404, detail=f"Folder for {report_type} not found or is not a directory.")
    
    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]
    if not csv_files:
        raise HTTPException(status_code=404, detail=f"No CSV files found for {report_type}.")
    
    return csv_files

@app.get("/reports/{report_type}/{file_name}", response_model=List[Dict[str, Any]], summary="View Specific Report Data")
async def get_report_data(report_type: str, file_name: str):
    """
    Reads and returns the content of a specific CSV report file as JSON.
    Handles various CSV formats and skips initial metadata rows.
    """
    if report_type not in REPORT_FOLDERS:
        raise HTTPException(status_code=404, detail="Report type not found.")
    
    folder_path = REPORT_FOLDERS[report_type]
    file_path = os.path.join(folder_path, file_name)

    if not os.path.exists(file_path) or not os.path.isfile(file_path):
        raise HTTPException(status_code=404, detail="File not found.")
    
    try:
        df = read_csv_with_flexible_encoding(file_path)
        # Convert DataFrame to a list of dictionaries (JSON format)
        return df.to_dict(orient="records")
    except ValueError as e:
        raise HTTPException(status_code=500, detail=f"Error reading CSV: {e}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"An unexpected error occurred: {e}")

# --- Run the application (for development/testing) ---
# To run this, save it as main.py and execute: uvicorn main:app --reload
# You will need to install uvicorn: pip install uvicorn
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)

========================================

### File: process_financial_data.py ###
import pandas as pd
import sqlite3
import os
import glob
from pathlib import Path

# --- Configuration ---
# Databases
METADATA_DB_PATH = "mufap_data.db"
FINANCIAL_DB_PATH = "financial_data.db"

# Directories to scan for data files
DIRECTORIES_TO_SCAN = ["PKFRV", "PKRV"]
MAX_HEADER_SCAN_ROWS = 15

# --- Format 1: Mutual Fund Contribution (PKFRV) Configuration ---
PKFRV_CORE_COLUMNS = {
   'Issue Date', 'Maturity date', 'Coupon Frequency'
}

# --- Format 2: Tenor Rate (PKRV) Configuration ---
PKRV_EXACT_COLUMNS = {'Tenor', 'Mid Rate', 'Change'}


def load_metadata_cache(db_path):
    """
    Loads report metadata (date, title) from the mufap_data.db into a cache.
    The cache is a dictionary mapping a filename to its metadata.
    """
    print(f"--- Loading metadata from '{db_path}' ---")
    if not os.path.exists(db_path):
        print(f"[Warning] Metadata database not found at '{db_path}'. Dates will not be added.")
        return {}

    cache = {}
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT filepath, date, title FROM mufap_reports")
        for row in cursor.fetchall():
            filepath, date, title = row
            if filepath:
                # Use the filename as the key for easy lookup
                filename = os.path.basename(filepath)
                cache[filename] = {'date': date, 'title': title}
        conn.close()
        print(f"Successfully loaded metadata for {len(cache)} reports.")
    except Exception as e:
        print(f"[Error] Could not load metadata from '{db_path}': {e}")
    return cache

def setup_database(db_path):
    """
    Sets up the new financial_data.db.
    Deletes old db and creates the tenor_rates table with new metadata columns.
    """
    if os.path.exists(db_path):
        os.remove(db_path)
        print(f"\nRemoved existing database '{db_path}'.")

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # Create the tenor_rates table with added metadata columns
    cursor.execute('''
        CREATE TABLE tenor_rates (
            unique_id INTEGER PRIMARY KEY AUTOINCREMENT,
            Tenor TEXT,
            "Mid Rate" REAL,
            Change REAL,
            report_date TEXT,
            source_filepath TEXT
        )
    ''')
    print("Table 'tenor_rates' created successfully.")
    conn.commit()
    return conn

def find_header_and_type(filepath):
    """
    Scans the first few rows of a file to find a header and determine its format.
    Standardizes core/exact columns to Title Case for matching.
    """
    file_extension = Path(filepath).suffix.lower()
    df_preview = None
    try:
        if file_extension == '.csv':
            df_preview = pd.read_csv(filepath, header=None, nrows=MAX_HEADER_SCAN_ROWS, sep=None, engine='python')
        elif file_extension in ['.xlsx', '.xls']:
            df_preview = pd.read_excel(filepath, header=None, nrows=MAX_HEADER_SCAN_ROWS, engine='openpyxl')
        else:
            return None, None
    except Exception as e:
        print(f"  [Error] Could not read preview from {Path(filepath).name}: {e}")
        return None, None

    # Standardize the required column sets to Title Case for reliable matching
    pkfrv_core_titlecase = {col.title() for col in PKFRV_CORE_COLUMNS}
    pkrv_exact_titlecase = {col.title() for col in PKRV_EXACT_COLUMNS}

    for i, row in df_preview.iterrows():
        # Clean potential header and convert to Title Case
        potential_header_list = [str(col).strip() for col in row.dropna()]
        potential_header_set = {col.title() for col in potential_header_list}

        if potential_header_set == pkrv_exact_titlecase:
            return i, 'PKRV'
        if pkfrv_core_titlecase.issubset(potential_header_set):
            return i, 'PKFRV'

    return None, None

def read_data_with_header(filepath, header_index):
    """Reads the full file using the identified header row index."""
    file_extension = Path(filepath).suffix.lower()
    try:
        if file_extension == '.csv':
            return pd.read_csv(filepath, skiprows=header_index, sep=None, engine='python')
        elif file_extension in ['.xlsx', '.xls']:
            return pd.read_excel(filepath, skiprows=header_index, engine='openpyxl')
    except Exception as e:
        print(f"  [Error] Could not read full data from {Path(filepath).name}: {e}")
        return None

def main():
    """Main function to orchestrate the file processing and database loading."""
    # Load metadata from the old database first
    metadata_cache = load_metadata_cache(METADATA_DB_PATH)

    # Setup the new database for financial data
    conn = setup_database(FINANCIAL_DB_PATH)

    # --- Phase 1: Scan all files, categorize, and stage for loading ---
    pkfrv_data_to_load = []
    all_pkfrv_columns = set()
    column_frequency = {}

    all_files = []
    for directory in DIRECTORIES_TO_SCAN:
        if os.path.isdir(directory):
            all_files.extend(glob.glob(os.path.join(directory, '*.csv')))
            all_files.extend(glob.glob(os.path.join(directory, '*.xlsx')))
        else:
            print(f"\n[Warning] Directory '{directory}' not found. Skipping.")

    if not all_files:
        print(f"\n[Warning] No .csv or .xlsx files found in specified directories.")
        conn.close()
        return

    print(f"\n--- Analyzing {len(all_files)} files ---")

    for filepath in all_files:
        filename = os.path.basename(filepath)
        print(f"\nProcessing file: {filename}")

        header_index, file_type = find_header_and_type(filepath)
        metadata = metadata_cache.get(filename, {'date': 'N/A', 'title': 'N/A'})

        if file_type == 'PKRV':
            if Path(filepath).suffix.lower() != '.csv':
                print(f"  -> Skipping: Tenor Rate (PKRV) format must be a .csv file.")
                continue

            print(f"  -> Identified as: Tenor Rate File (PKRV). Header on row {header_index + 1}.")
            df = read_data_with_header(filepath, header_index)
            if df is not None:
                # *** FIX: Standardize column names to prevent loading errors ***
                df.columns = [str(c).strip().title() for c in df.columns]
                
                # *** NEW: Add metadata columns ***
                df['report_date'] = metadata['date']
                df['source_filepath'] = filepath

                try:
                    df_to_load = df[['Tenor', 'Mid Rate', 'Change', 'report_date', 'source_filepath']]
                    df_to_load.to_sql('tenor_rates', conn, if_exists='append', index=False)
                    print(f"  -> Successfully loaded {len(df)} rows into 'tenor_rates'.")
                except Exception as e:
                    print(f"  [Error] Failed to load data into 'tenor_rates': {e}")

        elif file_type == 'PKFRV':
            print(f"  -> Identified as: Mutual Fund Contribution File (PKFRV). Header on row {header_index + 1}.")
            df = read_data_with_header(filepath, header_index)
            if df is not None:
                # *** FIX & NEW: Standardize columns and add metadata ***
                df.columns = [str(c).strip().title() for c in df.columns]
                df['Report_Date'] = metadata['date']
                df['Source_Filepath'] = filepath

                pkfrv_data_to_load.append(df)
                current_columns = set(df.columns)
                all_pkfrv_columns.update(current_columns)

                for col in current_columns:
                    column_frequency[col] = column_frequency.get(col, 0) + 1

                print(f"  -> Staged {len(df)} rows for batch loading into 'mutual_fund_data'.")
        else:
            print(f"  -> Skipping: Does not match any known format.")

    # --- Phase 2: Batch Load Mutual Fund Data ---
    if pkfrv_data_to_load:
        print("\n--- Batch Loading Mutual Fund Data ---")
        sorted_columns = sorted(list(all_pkfrv_columns))

        cursor = conn.cursor()
        cols_sql = ", ".join([f'"{col}"' for col in sorted_columns])
        create_table_sql = f"CREATE TABLE mutual_fund_data ({cols_sql})"
        
        # This will now succeed because of the standardized column names
        cursor.execute(create_table_sql)
        print(f"  -> Table 'mutual_fund_data' created with {len(sorted_columns)} unique columns.")

        total_rows_loaded = 0
        for i, df in enumerate(pkfrv_data_to_load, 1):
            for col in sorted_columns:
                if col not in df.columns:
                    df[col] = 0 # Insert 0 for missing data
            df_to_load = df[sorted_columns]
            df_to_load.to_sql('mutual_fund_data', conn, if_exists='append', index=False)
            total_rows_loaded += len(df_to_load)
        print(f"  -> Successfully loaded a total of {total_rows_loaded} rows.")

    # --- Phase 3: Column Frequency Report ---
    if column_frequency:
        print("\n--- Mutual Fund Data Column Frequency Report ---")
        sorted_report = sorted(column_frequency.items(), key=lambda item: item[0])
        for col, count in sorted_report:
            print(f"  - Column '{col}': Appeared in {count} file(s)")

    conn.close()
    print(f"\n--- Script Complete. Database saved to '{FINANCIAL_DB_PATH}' ---")

if __name__ == "__main__":
    main()

========================================

### File: readme.md ###
/html/body/div/div[5]/div[2]/div/div/div[2]/div/div/div[2]/div/div/div[2]/table/tbody/tr[13]/td[1]
/html/body/div/div[5]/div[2]/div/div/div[2]/div/div/div[2]/div/div/div[2]/table/tbody/tr[13]/td[2]
/html/body/div/div[5]/div[2]/div/div/div[2]/div/div/div[2]/div/div/div[2]/table/tbody/tr[14]/td[3]/a

========================================

### File: requirements.txt ###
fastapi
uvicorn[standard]
pandas
Jinja2

========================================

### File: shahmir_files.txt ###
Project Scan Report
Root Directory: C:\Users\pc\Desktop\shahmir
Platform: win32
Custom skip directories: PKRV, PKISRV, PKFRV
==================================================


### File: main.py ###
# main.py
from fastapi import FastAPI, HTTPException
from fastapi.responses import HTMLResponse
from typing import List, Dict, Any
import pandas as pd
import os
import uvicorn # Required to run the FastAPI app

# --- Configuration ---
# Define the base directory where your report folders are located
BASE_DATA_DIR = "." # Current directory, adjust if your folders are elsewhere

REPORT_FOLDERS = {
    "PKISRV": os.path.join(BASE_DATA_DIR, "PKISRV"),
    "PKRV": os.path.join(BASE_DATA_DIR, "PKRV"),
    "PKFRV": os.path.join(BASE_DATA_DIR, "PKFRV")
}

app = FastAPI(
    title="MUFAP Reports Data Viewer",
    description="API to view and access MUFAP CSV report data.",
    version="1.0.0"
)

# --- Helper Function for CSV Reading ---
def read_csv_with_flexible_encoding(filepath: str) -> pd.DataFrame:
    """
    Reads a CSV file, attempting multiple common encodings.
    Skips initial rows that might contain metadata before the actual header.
    """
    encodings_to_try = ['utf-8', 'latin1', 'cp1252']
    
    # Heuristic: Try to find the actual header row if it's not the first row.
    # This is a common issue with reports that have titles/metadata at the top.
    # We'll try reading with different skiprows values.
    for skip_rows in range(5): # Try skipping 0 to 4 rows
        for encoding in encodings_to_try:
            try:
                # Attempt to read, assuming the first non-empty row after skipping is the header
                df = pd.read_csv(filepath, encoding=encoding, skiprows=skip_rows)
                
                # Check if the header looks like actual column names (not just a single long string)
                # If the first column name is a long string or looks like metadata, try skipping more
                if df.empty or len(df.columns) < 2 or (len(df.columns) == 1 and "Unnamed" in df.columns[0]):
                    continue # This header might still be part of metadata, try skipping more
                
                # Drop columns that are entirely unnamed (e.g., 'Unnamed: 0', 'Unnamed: 1')
                # These often result from irregular CSV formatting
                df = df.loc[:, ~df.columns.str.contains('^Unnamed', na=False)]
                
                # Drop rows where all values are NaN (empty rows)
                df.dropna(how='all', inplace=True)

                # Reset index after dropping rows
                df.reset_index(drop=True, inplace=True)

                # If the first column is still problematic, consider it as a sign of bad header detection
                # and continue to the next skip_rows attempt.
                # This is a heuristic and might need fine-tuning for specific files.
                if not df.empty and df.columns[0].strip() == "":
                    continue

                return df
            except pd.errors.EmptyDataError:
                # File is empty or no data after skipping rows
                continue
            except UnicodeDecodeError:
                # Encoding failed, try next encoding
                continue
            except Exception as e:
                # Other pandas reading errors, print and try next encoding/skip_rows
                print(f"Error reading {filepath} with encoding {encoding} and skiprows {skip_rows}: {e}")
                continue
    
    raise ValueError(f"Could not read CSV file: {filepath} with any of the tried encodings/skiprows.")


# --- API Endpoints ---

@app.get("/", response_class=HTMLResponse, summary="Home Page")
async def read_root():
    """
    Provides a simple HTML home page with links to available report types.
    """
    html_content = """
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>MUFAP Reports Viewer</title>
        <script src="https://cdn.tailwindcss.com"></script>
        <style>
            body { font-family: 'Inter', sans-serif; }
        </style>
    </head>
    <body class="bg-gray-100 min-h-screen flex flex-col items-center justify-center p-4">
        <div class="bg-white p-8 rounded-lg shadow-lg max-w-md w-full text-center">
            <h1 class="text-3xl font-bold text-gray-800 mb-6">Welcome to MUFAP Reports Viewer</h1>
            <p class="text-gray-600 mb-8">Select a report type to view available files:</p>
            <div class="space-y-4">
                <a href="/reports/PKISRV" class="block bg-blue-500 hover:bg-blue-600 text-white font-semibold py-3 px-6 rounded-lg shadow-md transition duration-300 ease-in-out transform hover:scale-105">
                    View PKISRV Reports
                </a>
                <a href="/reports/PKRV" class="block bg-green-500 hover:bg-green-600 text-white font-semibold py-3 px-6 rounded-lg shadow-md transition duration-300 ease-in-out transform hover:scale-105">
                    View PKRV Reports
                </a>
                <a href="/reports/PKFRV" class="block bg-purple-500 hover:bg-purple-600 text-white font-semibold py-3 px-6 rounded-lg shadow-md transition duration-300 ease-in-out transform hover:scale-105">
                    View PKFRV Reports
                </a>
            </div>
            <p class="text-sm text-gray-500 mt-8">Note: Data consistency varies across files. Pre-processing is recommended for structured analysis.</p>
        </div>
    </body>
    </html>
    """
    return HTMLResponse(content=html_content)

@app.get("/reports/{report_type}", response_model=List[str], summary="List Files by Report Type")
async def list_files(report_type: str):
    """
    Lists all available CSV file names for a given report type.
    """
    if report_type not in REPORT_FOLDERS:
        raise HTTPException(status_code=404, detail="Report type not found.")
    
    folder_path = REPORT_FOLDERS[report_type]
    if not os.path.exists(folder_path) or not os.path.isdir(folder_path):
        raise HTTPException(status_code=404, detail=f"Folder for {report_type} not found or is not a directory.")
    
    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]
    if not csv_files:
        raise HTTPException(status_code=404, detail=f"No CSV files found for {report_type}.")
    
    return csv_files

@app.get("/reports/{report_type}/{file_name}", response_model=List[Dict[str, Any]], summary="View Specific Report Data")
async def get_report_data(report_type: str, file_name: str):
    """
    Reads and returns the content of a specific CSV report file as JSON.
    Handles various CSV formats and skips initial metadata rows.
    """
    if report_type not in REPORT_FOLDERS:
        raise HTTPException(status_code=404, detail="Report type not found.")
    
    folder_path = REPORT_FOLDERS[report_type]
    file_path = os.path.join(folder_path, file_name)

    if not os.path.exists(file_path) or not os.path.isfile(file_path):
        raise HTTPException(status_code=404, detail="File not found.")
    
    try:
        df = read_csv_with_flexible_encoding(file_path)
        # Convert DataFrame to a list of dictionaries (JSON format)
        return df.to_dict(orient="records")
    except ValueError as e:
        raise HTTPException(status_code=500, detail=f"Error reading CSV: {e}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"An unexpected error occurred: {e}")

# --- Run the application (for development/testing) ---
# To run this, save it as main.py and execute: uvicorn main:app --reload
# You will need to install uvicorn: pip install uvicorn
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)

========================================

### File: process_financial_data.py ###
import pandas as pd
import sqlite3
import os
import glob
from pathlib import Path

# --- Configuration ---
# Databases
METADATA_DB_PATH = "mufap_data.db"
FINANCIAL_DB_PATH = "financial_data.db"

# Directories to scan for data files
DIRECTORIES_TO_SCAN = ["PKFRV", "PKRV"]
MAX_HEADER_SCAN_ROWS = 15

# --- Format 1: Mutual Fund Contribution (PKFRV) Configuration ---
PKFRV_CORE_COLUMNS = {
   'Issue Date', 'Maturity date', 'Coupon Frequency'
}

# --- Format 2: Tenor Rate (PKRV) Configuration ---
PKRV_EXACT_COLUMNS = {'Tenor', 'Mid Rate', 'Change'}


def load_metadata_cache(db_path):
    """
    Loads report metadata (date, title) from the mufap_data.db into a cache.
    The cache is a dictionary mapping a filename to its metadata.
    """
    print(f"--- Loading metadata from '{db_path}' ---")
    if not os.path.exists(db_path):
        print(f"[Warning] Metadata database not found at '{db_path}'. Dates will not be added.")
        return {}

    cache = {}
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT filepath, date, title FROM mufap_reports")
        for row in cursor.fetchall():
            filepath, date, title = row
            if filepath:
                # Use the filename as the key for easy lookup
                filename = os.path.basename(filepath)
                cache[filename] = {'date': date, 'title': title}
        conn.close()
        print(f"Successfully loaded metadata for {len(cache)} reports.")
    except Exception as e:
        print(f"[Error] Could not load metadata from '{db_path}': {e}")
    return cache

def setup_database(db_path):
    """
    Sets up the new financial_data.db.
    Deletes old db and creates the tenor_rates table with new metadata columns.
    """
    if os.path.exists(db_path):
        os.remove(db_path)
        print(f"\nRemoved existing database '{db_path}'.")

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # Create the tenor_rates table with added metadata columns
    cursor.execute('''
        CREATE TABLE tenor_rates (
            unique_id INTEGER PRIMARY KEY AUTOINCREMENT,
            Tenor TEXT,
            "Mid Rate" REAL,
            Change REAL,
            report_date TEXT,
            source_filepath TEXT
        )
    ''')
    print("Table 'tenor_rates' created successfully.")
    conn.commit()
    return conn

def find_header_and_type(filepath):
    """
    Scans the first few rows of a file to find a header and determine its format.
    Standardizes core/exact columns to Title Case for matching.
    """
    file_extension = Path(filepath).suffix.lower()
    df_preview = None
    try:
        if file_extension == '.csv':
            df_preview = pd.read_csv(filepath, header=None, nrows=MAX_HEADER_SCAN_ROWS, sep=None, engine='python')
        elif file_extension in ['.xlsx', '.xls']:
            df_preview = pd.read_excel(filepath, header=None, nrows=MAX_HEADER_SCAN_ROWS, engine='openpyxl')
        else:
            return None, None
    except Exception as e:
        print(f"  [Error] Could not read preview from {Path(filepath).name}: {e}")
        return None, None

    # Standardize the required column sets to Title Case for reliable matching
    pkfrv_core_titlecase = {col.title() for col in PKFRV_CORE_COLUMNS}
    pkrv_exact_titlecase = {col.title() for col in PKRV_EXACT_COLUMNS}

    for i, row in df_preview.iterrows():
        # Clean potential header and convert to Title Case
        potential_header_list = [str(col).strip() for col in row.dropna()]
        potential_header_set = {col.title() for col in potential_header_list}

        if potential_header_set == pkrv_exact_titlecase:
            return i, 'PKRV'
        if pkfrv_core_titlecase.issubset(potential_header_set):
            return i, 'PKFRV'

    return None, None

def read_data_with_header(filepath, header_index):
    """Reads the full file using the identified header row index."""
    file_extension = Path(filepath).suffix.lower()
    try:
        if file_extension == '.csv':
            return pd.read_csv(filepath, skiprows=header_index, sep=None, engine='python')
        elif file_extension in ['.xlsx', '.xls']:
            return pd.read_excel(filepath, skiprows=header_index, engine='openpyxl')
    except Exception as e:
        print(f"  [Error] Could not read full data from {Path(filepath).name}: {e}")
        return None

def main():
    """Main function to orchestrate the file processing and database loading."""
    # Load metadata from the old database first
    metadata_cache = load_metadata_cache(METADATA_DB_PATH)

    # Setup the new database for financial data
    conn = setup_database(FINANCIAL_DB_PATH)

    # --- Phase 1: Scan all files, categorize, and stage for loading ---
    pkfrv_data_to_load = []
    all_pkfrv_columns = set()
    column_frequency = {}

    all_files = []
    for directory in DIRECTORIES_TO_SCAN:
        if os.path.isdir(directory):
            all_files.extend(glob.glob(os.path.join(directory, '*.csv')))
            all_files.extend(glob.glob(os.path.join(directory, '*.xlsx')))
        else:
            print(f"\n[Warning] Directory '{directory}' not found. Skipping.")

    if not all_files:
        print(f"\n[Warning] No .csv or .xlsx files found in specified directories.")
        conn.close()
        return

    print(f"\n--- Analyzing {len(all_files)} files ---")

    for filepath in all_files:
        filename = os.path.basename(filepath)
        print(f"\nProcessing file: {filename}")

        header_index, file_type = find_header_and_type(filepath)
        metadata = metadata_cache.get(filename, {'date': 'N/A', 'title': 'N/A'})

        if file_type == 'PKRV':
            if Path(filepath).suffix.lower() != '.csv':
                print(f"  -> Skipping: Tenor Rate (PKRV) format must be a .csv file.")
                continue

            print(f"  -> Identified as: Tenor Rate File (PKRV). Header on row {header_index + 1}.")
            df = read_data_with_header(filepath, header_index)
            if df is not None:
                # *** FIX: Standardize column names to prevent loading errors ***
                df.columns = [str(c).strip().title() for c in df.columns]
                
                # *** NEW: Add metadata columns ***
                df['report_date'] = metadata['date']
                df['source_filepath'] = filepath

                try:
                    df_to_load = df[['Tenor', 'Mid Rate', 'Change', 'report_date', 'source_filepath']]
                    df_to_load.to_sql('tenor_rates', conn, if_exists='append', index=False)
                    print(f"  -> Successfully loaded {len(df)} rows into 'tenor_rates'.")
                except Exception as e:
                    print(f"  [Error] Failed to load data into 'tenor_rates': {e}")

        elif file_type == 'PKFRV':
            print(f"  -> Identified as: Mutual Fund Contribution File (PKFRV). Header on row {header_index + 1}.")
            df = read_data_with_header(filepath, header_index)
            if df is not None:
                # *** FIX & NEW: Standardize columns and add metadata ***
                df.columns = [str(c).strip().title() for c in df.columns]
                df['Report_Date'] = metadata['date']
                df['Source_Filepath'] = filepath

                pkfrv_data_to_load.append(df)
                current_columns = set(df.columns)
                all_pkfrv_columns.update(current_columns)

                for col in current_columns:
                    column_frequency[col] = column_frequency.get(col, 0) + 1

                print(f"  -> Staged {len(df)} rows for batch loading into 'mutual_fund_data'.")
        else:
            print(f"  -> Skipping: Does not match any known format.")

    # --- Phase 2: Batch Load Mutual Fund Data ---
    if pkfrv_data_to_load:
        print("\n--- Batch Loading Mutual Fund Data ---")
        sorted_columns = sorted(list(all_pkfrv_columns))

        cursor = conn.cursor()
        cols_sql = ", ".join([f'"{col}"' for col in sorted_columns])
        create_table_sql = f"CREATE TABLE mutual_fund_data ({cols_sql})"
        
        # This will now succeed because of the standardized column names
        cursor.execute(create_table_sql)
        print(f"  -> Table 'mutual_fund_data' created with {len(sorted_columns)} unique columns.")

        total_rows_loaded = 0
        for i, df in enumerate(pkfrv_data_to_load, 1):
            for col in sorted_columns:
                if col not in df.columns:
                    df[col] = 0 # Insert 0 for missing data
            df_to_load = df[sorted_columns]
            df_to_load.to_sql('mutual_fund_data', conn, if_exists='append', index=False)
            total_rows_loaded += len(df_to_load)
        print(f"  -> Successfully loaded a total of {total_rows_loaded} rows.")

    # --- Phase 3: Column Frequency Report ---
    if column_frequency:
        print("\n--- Mutual Fund Data Column Frequency Report ---")
        sorted_report = sorted(column_frequency.items(), key=lambda item: item[0])
        for col, count in sorted_report:
            print(f"  - Column '{col}': Appeared in {count} file(s)")

    conn.close()
    print(f"\n--- Script Complete. Database saved to '{FINANCIAL_DB_PATH}' ---")

if __name__ == "__main__":
    main()

========================================

### File: try2.py ###
import requests
import json
import sqlite3
import os
import re
from datetime import datetime
import asyncio
import aiohttp # For asynchronous HTTP requests
import aiofiles # For asynchronous file operations (optional, but good practice with asyncio)

# --- Configuration ---
API_URL = "https://mufap.com.pk/WebRegulations/GetSecpFileById"
BASE_FILE_DOWNLOAD_URL = "https://mufap.com.pk"
DATABASE_NAME = "mufap_data.db"
REPORT_FOLDERS = {
    "PKISRV": "PKISRV",
    "PKRV": "PKRV",
    "PKFRV": "PKFRV"
}
# Ensure these folders exist
for folder in REPORT_FOLDERS.values():
    os.makedirs(folder, exist_ok=True)

# --- Database Functions ---
# SQLite operations are synchronous, but we'll manage connections carefully.
def connect_db():
    """Establishes a connection to the SQLite database."""
    try:
        conn = sqlite3.connect(DATABASE_NAME)
        conn.row_factory = sqlite3.Row # Allows accessing columns by name
        return conn
    except sqlite3.Error as e:
        print(f"Database connection error: {e}")
        return None

def create_table(conn):
    """Creates the mufap_reports table if it doesn't exist."""
    try:
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS mufap_reports (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                timeofadding DATETIME DEFAULT CURRENT_TIMESTAMP,
                date TEXT,
                title TEXT,
                filepath TEXT UNIQUE, -- Added UNIQUE constraint for FilePath
                report_type TEXT,
                fk_header_submenu_tab_id INTEGER
            )
        ''')
        conn.commit()
        print("Table 'mufap_reports' ensured to exist.")
    except sqlite3.Error as e:
        print(f"Error creating table: {e}")

def report_exists_in_db(conn, filepath):
    """Checks if a report with the given filepath already exists in the database."""
    cursor = conn.cursor()
    cursor.execute("SELECT 1 FROM mufap_reports WHERE filepath = ?", (filepath,))
    return cursor.fetchone() is not None

def insert_report_data(conn, report_data):
    """Inserts a single report's metadata into the database, preventing duplicates."""
    filepath = report_data.get('FilePath')
    if not filepath:
        print(f"Skipping insertion for report with no FilePath: {report_data.get('Title')}")
        return False

    if report_exists_in_db(conn, filepath):
        print(f"Skipping DB insertion: Report already exists for FilePath: {filepath}")
        return False

    try:
        cursor = conn.cursor()
        cursor.execute('''
            INSERT INTO mufap_reports (date, title, filepath, report_type, fk_header_submenu_tab_id)
            VALUES (?, ?, ?, ?, ?)
        ''', (report_data['Date'], report_data['Title'], report_data['FilePath'], report_data['report_type'], report_data['fk_HeaderSubMenuTabId']))
        conn.commit()
        print(f"Inserted: {report_data['Title']} (fk_HeaderSubMenuTabId: {report_data['fk_HeaderSubMenuTabId']})")
        return True
    except sqlite3.Error as e:
        print(f"Error inserting data for {report_data['Title']}: {e}")
        return False

# --- Helper Functions ---
def parse_dotnet_date(date_string):
    """
    Parses a .NET /Date(...) string and returns a formatted datetime string.
    Example: /Date(1580238000000)/ -> 2020-01-28 11:00:00
    """
    match = re.search(r'\/Date\((\d+)\)\/', date_string)
    if match:
        timestamp_ms = int(match.group(1))
        # Convert milliseconds to seconds
        dt_object = datetime.fromtimestamp(timestamp_ms / 1000)
        return dt_object.strftime('%Y-%m-%d %H:%M:%S')
    return date_string # Return original if not in expected format

def get_report_type(title):
    """Determines the report type based on the title."""
    title_upper = title.upper()
    if "PKISRV" in title_upper:
        return "PKISRV"
    elif "PKRV" in title_upper:
        return "PKRV"
    elif "PKFRV" in title_upper:
        return "PKFRV"
    return "UNKNOWN"

async def download_file(session, file_url, save_path):
    """Downloads a file from a given URL and saves it to a specified path asynchronously."""
    if os.path.exists(save_path):
        print(f"Skipping download: File already exists at: {save_path}")
        return True

    try:
        print(f"Attempting to download: {file_url}")
        async with session.get(file_url) as response:
            response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)

            # Using aiofiles for asynchronous file writing
            async with aiofiles.open(save_path, 'wb') as f:
                async for chunk in response.content.iter_chunked(8192):
                    await f.write(chunk)
        print(f"Successfully downloaded and saved to: {save_path}")
        return True
    except aiohttp.ClientError as e:
        print(f"Error downloading {file_url}: {e}")
        return False
    except Exception as e:
        print(f"An unexpected error occurred during download of {file_url}: {e}")
        return False

async def process_single_report(session, conn, report_data_raw, fk_header_submenu_tab_id):
    """Processes a single report: parses data, inserts into DB, and downloads file."""
    # Create a mutable copy of report_data_raw
    report = report_data_raw.copy()

    # Process metadata
    report['Date'] = parse_dotnet_date(report.get('Date', ''))
    report['report_type'] = get_report_type(report.get('Title', ''))
    report['fk_HeaderSubMenuTabId'] = fk_header_submenu_tab_id # Add the ID to the report data

    # Insert into DB (synchronous, but fast enough for individual inserts)
    if insert_report_data(conn, report):
        # Download file
        file_path_suffix = report.get('FilePath')
        if file_path_suffix:
            full_file_url = BASE_FILE_DOWNLOAD_URL + file_path_suffix
            
            # Determine save directory
            report_type_folder = REPORT_FOLDERS.get(report['report_type'], 'UNKNOWN_REPORTS')
            os.makedirs(report_type_folder, exist_ok=True) # Ensure folder exists
            
            # Extract filename from FilePath
            file_name = os.path.basename(file_path_suffix)
            save_path = os.path.join(report_type_folder, file_name)
            
            await download_file(session, full_file_url, save_path)
        else:
            print(f"No FilePath found for report: {report.get('Title')}")
    else:
        print(f"Skipping file download for {report.get('Title')} due to DB insertion being skipped or failed.")


# --- Main Scraper Logic ---
async def scrape_mufap_reports(fk_header_submenu_tab_id):
    """
    Main asynchronous function to fetch reports, store metadata in DB, and download files.
    Accepts fk_header_submenu_tab_id as an argument.
    """
    conn = connect_db()
    if not conn:
        return

    create_table(conn)

    headers = {
        'Content-Type': 'application/json',
        'Referer': 'https://mufap.com.pk/',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',
    }
    payload = {'fk_HeaderSubMenuTabId': fk_header_submenu_tab_id}

    print(f"\n--- Fetching data for fk_HeaderSubMenuTabId: {fk_header_submenu_tab_id} ---")
    
    async with aiohttp.ClientSession(headers=headers) as session:
        try:
            async with session.post(API_URL, json=payload) as response:
                response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)
                
                data_response = await response.json()

            # Check for the "No data found" message but continue to process the 'data' field
            if data_response.get("statusCode") == "00" and data_response.get("message") == "No data found":
                print(f"API returned 'No data found' message for fk_HeaderSubMenuTabId: {fk_header_submenu_tab_id}. Still checking 'data' field...")

            reports = data_response.get("data", [])
            if not reports:
                print(f"No reports found in the 'data' field of the response for fk_header_submenu_tab_id: {fk_header_submenu_tab_id}.")
                return # Exit if 'data' is truly empty

            print(f"Found {len(reports)} reports for fk_header_submenu_tab_id: {fk_header_submenu_tab_id}.")
            
            # Create a list of tasks to run concurrently
            tasks = []
            for report_data_raw in reports:
                tasks.append(process_single_report(session, conn, report_data_raw, fk_header_submenu_tab_id))
            
            # Run all tasks concurrently
            await asyncio.gather(*tasks)

        except aiohttp.ClientError as e:
            print(f"Error during API call for fk_header_submenu_tab_id {fk_header_submenu_tab_id}: {e}")
        except json.JSONDecodeError:
            print(f"Failed to decode JSON response from API for fk_header_submenu_tab_id {fk_header_submenu_tab_id}.")
        except Exception as e:
            print(f"An unexpected error occurred during scraping: {e}")
        finally:
            if conn:
                conn.close()
                print("Database connection closed.")

# --- Execute the scraper ---
if __name__ == "__main__":
    # Ensure aiofiles is installed: pip install aiofiles
    # Ensure aiohttp is installed: pip install aiohttp

    # Only try the specific ID 46 as requested
    asyncio.run(scrape_mufap_reports(46)) 

    print("\n--- Scraper run complete ---")

========================================

### File: verify_columns.py ###
import os
import pandas as pd
from collections import Counter

# --- Configuration ---
# Define the base directory where your report folders are located
BASE_DATA_DIR = "." # Current directory, adjust if your folders are elsewhere

REPORT_FOLDERS = {
    "PKISRV": os.path.join(BASE_DATA_DIR, "PKISRV"),
    "PKRV": os.path.join(BASE_DATA_DIR, "PKRV"),
    "PKFRV": os.path.join(BASE_DATA_DIR, "PKFRV")
}

# --- Helper Functions ---
def get_csv_header(filepath: str) -> list[str] | None:
    """
    Reads the header (first row) of a CSV file and returns it as a list of column names.
    Uses pandas to robustly read CSVs, handling various delimiters and quoting.
    Tries multiple encodings and skiprows values to find the actual header.
    """
    encodings_to_try = ['utf-8', 'latin1', 'cp1252']
    
    for skip_rows in range(5): # Try skipping 0 to 4 rows
        for encoding in encodings_to_try:
            try:
                # Attempt to read, assuming the first non-empty row after skipping is the header
                df = pd.read_csv(filepath, encoding=encoding, skiprows=skip_rows, nrows=0)
                
                # Check if the header looks like actual column names (not just a single long string)
                # If the first column name is a long string or looks like metadata, try skipping more
                if df.empty or len(df.columns) < 1: # Must have at least one column
                    continue 
                
                # Drop columns that are entirely unnamed (e.g., 'Unnamed: 0', 'Unnamed: 1')
                # These often result from irregular CSV formatting
                # Note: This is applied to the *header detection logic*, not the actual data load.
                # It helps in standardizing the detected header for comparison.
                cleaned_columns = [col for col in df.columns if not str(col).strip().lower().startswith('unnamed:')]
                
                # If after cleaning, there are very few columns or they still look like metadata,
                # it might mean the header is still not correctly identified.
                # This is a heuristic and might need fine-tuning for specific files.
                if not cleaned_columns: # If all columns were 'Unnamed' or empty, this might not be the real header
                    continue
                
                return cleaned_columns # Return the cleaned list of column names
            except pd.errors.EmptyDataError:
                # File is empty or no data after skipping rows
                continue
            except UnicodeDecodeError:
                # Encoding failed, try next encoding
                continue
            except Exception as e:
                # Other pandas reading errors, print and try next encoding/skip_rows
                # print(f"Debug: Error reading {filepath} with encoding {encoding} and skiprows {skip_rows}: {e}")
                continue
    
    # print(f"Warning: Could not read header from {filepath} with any of the tried encodings/skiprows.")
    return None

# --- Main Analysis Logic ---
def analyze_csv_columns():
    """
    Analyzes column structures of CSV files within each report folder,
    identifying the most common and all unique combinations.
    """
    print("--- Starting CSV Column Structure Analysis ---")

    for report_type, folder_path in REPORT_FOLDERS.items():
        print(f"\nAnalyzing folder: {folder_path} ({report_type} reports)")

        if not os.path.exists(folder_path):
            print(f"Folder does not exist: {folder_path}. Skipping.")
            continue
        if not os.path.isdir(folder_path):
            print(f"Path is not a directory: {folder_path}. Skipping.")
            continue

        csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]

        if not csv_files:
            print(f"No CSV files found in {folder_path}.")
            continue

        # Use Counter to count occurrences of each unique header structure
        # Convert list of columns to a tuple so it's hashable for Counter
        header_counts = Counter()
        file_to_header_map = {} # To store which files map to which header structure

        for filename in csv_files:
            filepath = os.path.join(folder_path, filename)
            header = get_csv_header(filepath)
            if header is not None:
                header_tuple = tuple(header) # Convert list to tuple for hashing
                header_counts[header_tuple] += 1
                if header_tuple not in file_to_header_map:
                    file_to_header_map[header_tuple] = []
                file_to_header_map[header_tuple].append(filename)
            else:
                print(f"Could not determine header for file: {filename}. Skipping from analysis.")

        if not header_counts:
            print(f"No valid headers extracted from any CSV in {folder_path}.")
            continue

        print(f"\n--- Column Structure Summary for {report_type} ---")
        
        # Find the most common header structure
        most_common_header, most_common_count = header_counts.most_common(1)[0]
        print(f"Most Common Structure ({most_common_count} files):")
        print(f"  Columns: {list(most_common_header)}")
        print(f"  Example files: {file_to_header_map.get(most_common_header, [])[:3]}...") # Show first 3 examples

        print("\nAll Unique Column Structures and Their Counts:")
        for header_tuple, count in header_counts.most_common(): # Iterate in descending order of frequency
            print(f"  Count: {count}")
            print(f"  Columns: {list(header_tuple)}")
            # Optionally, list some files for each structure
            # print(f"  Files: {file_to_header_map.get(header_tuple, [])[:2]}...") # Show first 2 examples
            print("-" * 30)

    print("\n--- CSV Column Structure Analysis Complete ---")

# --- Execute the script ---
if __name__ == "__main__":
    # Ensure pandas is installed: pip install pandas
    analyze_csv_columns()

========================================

### File: viewer_app.py ###
import pandas as pd
import sqlite3
import os
from fastapi import FastAPI, HTTPException
from fastapi.responses import HTMLResponse, FileResponse
from jinja2 import Template

# --- Configuration ---
DATABASE_PATH = "financial_data.db"
# This is the base directory where your PKFRV/PKRV folders are located.
# The app needs this to construct the correct download paths.
FILES_BASE_DIRECTORY = "." 

# --- FastAPI App Initialization ---
app = FastAPI(
    title="Financial Data Viewer",
    description="A modern web interface to view and download financial data.",
    version="1.0.0", port =7484
)

# --- HTML Templates ---
# Using Jinja2 templates directly in the script for simplicity.

HTML_TEMPLATE = Template("""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Financial Data Viewer</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body { font-family: 'Inter', sans-serif; }
        .table-container { max-height: 80vh; }
    </style>
</head>
<body class="bg-gray-50 text-gray-800">

    <div class="container mx-auto p-4 sm:p-6 lg:p-8">
        <header class="text-center mb-8">
            <h1 class="text-4xl font-bold text-gray-900">Financial Data Viewer</h1>
            <p class="text-lg text-gray-600 mt-2">Select a report type to view the data.</p>
        </header>

        <div class="flex justify-center gap-4 mb-8">
            <a href="/data/PKFRV" class="bg-blue-600 hover:bg-blue-700 text-white font-bold py-3 px-6 rounded-lg shadow-md transition-transform transform hover:scale-105">
                View Mutual Fund Data (PKFRV)
            </a>
            <a href="/data/PKRV" class="bg-green-600 hover:bg-green-700 text-white font-bold py-3 px-6 rounded-lg shadow-md transition-transform transform hover:scale-105">
                View Tenor Rates (PKRV)
            </a>
        </div>

        {% if data %}
        <div class="bg-white rounded-xl shadow-lg overflow-hidden">
            <div class="p-6 border-b border-gray-200">
                <h2 class="text-2xl font-semibold">Displaying {{ table_name }} Data</h2>
            </div>
            <div class="table-container overflow-x-auto">
                <table class="min-w-full divide-y divide-gray-200">
                    <thead class="bg-gray-50 sticky top-0">
                        <tr>
                            {% for header in headers %}
                            <th scope="col" class="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                                {{ header }}
                            </th>
                            {% endfor %}
                            <th scope="col" class="px-6 py-3 text-left text-xs font-medium text-gray-500 uppercase tracking-wider">
                                Action
                            </th>
                        </tr>
                    </thead>
                    <tbody class="bg-white divide-y divide-gray-200">
                        {% for row in data %}
                        <tr>
                            {% for cell in row %}
                            <td class="px-6 py-4 whitespace-nowrap text-sm text-gray-700">
                                {{ cell }}
                            </td>
                            {% endfor %}
                            <td class="px-6 py-4 whitespace-nowrap text-sm">
                                <a href="/download/{{ row[-1] }}" class="text-indigo-600 hover:text-indigo-900 font-medium" download>
                                    Download File
                                </a>
                            </td>
                        </tr>
                        {% endfor %}
                    </tbody>
                </table>
            </div>
        </div>
        {% endif %}
    </div>

</body>
</html>
""")

# --- Helper Functions ---

def db_connect():
    """Establishes a connection to the SQLite database."""
    if not os.path.exists(DATABASE_PATH):
        raise HTTPException(status_code=500, detail=f"Database file not found at '{DATABASE_PATH}'. Please run the processing script first.")
    try:
        conn = sqlite3.connect(DATABASE_PATH)
        return conn
    except sqlite3.Error as e:
        raise HTTPException(status_code=500, detail=f"Database connection error: {e}")

def get_clean_ordered_data(report_type: str):
    """
    Fetches data from the database, cleans unwanted columns, and orders them.
    """
    conn = db_connect()
    
    if report_type == "PKFRV":
        table_name = "mutual_fund_data"
        # Define the specific order for PKFRV columns
        fixed_order = ['Report_Date', 'Issue Date', 'Maturity Date', 'Coupon Frequency']
    elif report_type == "PKRV":
        table_name = "tenor_rates"
        # Define the specific order for PKRV columns
        fixed_order = ['report_date', 'Tenor', 'Mid Rate', 'Change']
    else:
        raise HTTPException(status_code=404, detail="Report type not found.")

    try:
        df = pd.read_sql_query(f"SELECT * FROM {table_name}", conn)
    except pd.io.sql.DatabaseError:
        raise HTTPException(status_code=404, detail=f"Table '{table_name}' not found in the database.")
    finally:
        conn.close()

    # Store the source filepath for the download link, then drop it
    source_filepaths = df['Source_Filepath'].copy() if 'Source_Filepath' in df.columns else df['source_filepath'].copy()

    # --- Column Filtering ---
    # List of columns to always exclude
    cols_to_exclude = {'unique_id', 'Source_Filepath', 'source_filepath', 'Report_Date', 'report_date', '﻿'}
    # Dynamically find 'Unnamed' columns
    unnamed_cols = {col for col in df.columns if 'unnamed' in col.lower()}
    cols_to_exclude.update(unnamed_cols)
    
    # Get the remaining data columns
    data_cols = [col for col in df.columns if col not in cols_to_exclude]

    # --- Column Ordering ---
    # Start with the fixed order, then add the rest of the data columns alphabetically
    final_ordered_cols = []
    for col in fixed_order:
        if col in df.columns:
            final_ordered_cols.append(col)
    
    remaining_cols = sorted([col for col in data_cols if col not in final_ordered_cols])
    final_ordered_cols.extend(remaining_cols)

    # Create the final DataFrame with ordered columns
    final_df = df[final_ordered_cols]
    
    # Add the source filepath back as the last column for the download link
    final_df['source_filepath_for_download'] = source_filepaths
    
    return final_df.values.tolist(), final_ordered_cols, table_name.replace('_', ' ').title()


# --- API Endpoints ---

@app.get("/", response_class=HTMLResponse)
async def read_root():
    """
    Serves the main landing page with buttons to select a report type.
    """
    return HTML_TEMPLATE.render()

@app.get("/data/{report_type}", response_class=HTMLResponse)
async def get_report_data(report_type: str):
    """
    Fetches, cleans, and displays data for the selected report type.
    """
    data, headers, table_name = get_clean_ordered_data(report_type.upper())
    return HTML_TEMPLATE.render(data=data, headers=headers, table_name=table_name)

@app.get("/download/{filepath:path}")
async def download_file(filepath: str):
    """
    Provides a file for download. The path is constructed relative to the base directory.
    """
    # Construct the full, safe path to the file
    full_path = os.path.join(FILES_BASE_DIRECTORY, filepath)
    
    if not os.path.exists(full_path):
        raise HTTPException(status_code=404, detail="File not found.")
        
    return FileResponse(path=full_path, filename=os.path.basename(full_path))

# To run this app:
# 1. Save it as viewer_app.py
# 2. Make sure financial_data.db is in the same directory.
# 3. Make sure your PKFRV and PKRV folders are also in the same directory.
# 4. Run in your terminal: uvicorn viewer_app:app --reload

========================================

### File: viewer_app_interactive.py ###
import pandas as pd
import sqlite3
import os
from fastapi import FastAPI, HTTPException
from fastapi.responses import HTMLResponse, FileResponse
from jinja2 import Template

# --- Configuration ---
DATABASE_PATH = "financial_data.db"
# This is the base directory where your PKFRV/PKRV folders are located.
FILES_BASE_DIRECTORY = "." 

# --- FastAPI App Initialization ---
app = FastAPI(
    title="Interactive Financial Data Viewer",
    description="An interactive web interface to view, sort, filter, and download financial data.",
    version="2.0.0"
)

# --- HTML Template with DataTables.js Integration ---
HTML_TEMPLATE = Template("""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Financial Data Viewer</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <!-- DataTables CSS -->
    <link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/1.13.6/css/jquery.dataTables.min.css">
    <style>
        body { font-family: 'Inter', sans-serif; }
        /* Style DataTables search box and length selector to match Tailwind UI */
        .dataTables_wrapper .dataTables_length, .dataTables_wrapper .dataTables_filter {
            margin-bottom: 1.5rem;
        }
        .dataTables_wrapper .dataTables_length select, .dataTables_wrapper .dataTables_filter input {
            padding: 0.5rem 0.75rem;
            border-radius: 0.5rem;
            border: 1px solid #d1d5db;
        }
        .dataTables_wrapper .dataTables_paginate .paginate_button {
            padding: 0.5em 1em;
        }
    </style>
</head>
<body class="bg-gray-50 text-gray-800">

    <div class="container mx-auto p-4 sm:p-6 lg:p-8">
        <header class="text-center mb-8">
            <h1 class="text-4xl font-bold text-gray-900">Interactive Data Viewer</h1>
            <p class="text-lg text-gray-600 mt-2">Select a report to search, sort, and filter data.</p>
        </header>

        <div class="flex justify-center gap-4 mb-8">
            <a href="/data/PKFRV" class="bg-blue-600 hover:bg-blue-700 text-white font-bold py-3 px-6 rounded-lg shadow-md transition-transform transform hover:scale-105">
                View Mutual Fund Data (PKFRV)
            </a>
            <a href="/data/PKRV" class="bg-green-600 hover:bg-green-700 text-white font-bold py-3 px-6 rounded-lg shadow-md transition-transform transform hover:scale-105">
                View Tenor Rates (PKRV)
            </a>
        </div>

        {% if data %}
        <div class="bg-white rounded-xl shadow-lg p-6">
            <h2 class="text-2xl font-semibold mb-4">Displaying {{ table_name }} Data</h2>
            <table id="dataTable" class="display min-w-full">
                <thead>
                    <tr>
                        {% for header in headers %}
                        <th class="text-left p-2">{{ header }}</th>
                        {% endfor %}
                        <th class="text-left p-2">Action</th>
                    </tr>
                </thead>
                <tbody>
                    {% for row in data %}
                    <tr>
                        {% for cell in row %}
                        <td class="p-2 border-t border-gray-200">{{ cell }}</td>
                        {% endfor %}
                        <td class="p-2 border-t border-gray-200">
                            <a href="/download/{{ row[-1] }}" class="text-indigo-600 hover:text-indigo-900 font-medium" download>
                                Download File
                            </a>
                        </td>
                    </tr>
                    {% endfor %}
                </tbody>
            </table>
        </div>
        {% endif %}
    </div>

    <!-- jQuery and DataTables JS -->
    <script src="https://code.jquery.com/jquery-3.7.0.js"></script>
    <script src="https://cdn.datatables.net/1.13.6/js/jquery.dataTables.min.js"></script>
    <script>
        // Initialize DataTables to enable interactive features
        $(document).ready(function() {
            $('#dataTable').DataTable();
        });
    </script>

</body>
</html>
""")

# --- Helper Functions ---

def db_connect():
    """Establishes a connection to the SQLite database."""
    if not os.path.exists(DATABASE_PATH):
        raise HTTPException(status_code=500, detail=f"Database file not found at '{DATABASE_PATH}'.")
    try:
        conn = sqlite3.connect(DATABASE_PATH)
        return conn
    except sqlite3.Error as e:
        raise HTTPException(status_code=500, detail=f"Database connection error: {e}")

def get_clean_ordered_data(report_type: str):
    """
    Fetches data, cleans columns, formats dates, and orders columns.
    """
    conn = db_connect()
    
    if report_type == "PKFRV":
        table_name = "mutual_fund_data"
        fixed_order = ['Report_Date', 'Issue Date', 'Maturity Date', 'Coupon Frequency']
    elif report_type == "PKRV":
        table_name = "tenor_rates"
        fixed_order = ['report_date', 'Tenor', 'Mid Rate', 'Change']
    else:
        raise HTTPException(status_code=404, detail="Report type not found.")

    try:
        df = pd.read_sql_query(f"SELECT * FROM {table_name}", conn)
    except pd.io.sql.DatabaseError:
        raise HTTPException(status_code=404, detail=f"Table '{table_name}' not found.")
    finally:
        conn.close()

    # --- Date Formatting ---
    # Convert any column that looks like a date/datetime into a simple YYYY-MM-DD string
    for col in df.columns:
        if df[col].dtype == 'object':
            try:
                # Attempt to convert to datetime and then format. Errors are ignored.
                df[col] = pd.to_datetime(df[col], errors='coerce').dt.strftime('%Y-%m-%d').fillna(df[col])
            except Exception:
                continue # Ignore columns that can't be converted

    # Store the source filepath for the download link
    source_filepaths = df['Source_Filepath'].copy() if 'Source_Filepath' in df.columns else df['source_filepath'].copy()

    # --- Column Filtering ---
    cols_to_exclude = {'unique_id', 'Source_Filepath', 'source_filepath', '﻿'}
    unnamed_cols = {col for col in df.columns if 'unnamed' in col.lower()}
    cols_to_exclude.update(unnamed_cols)
    
    data_cols = [col for col in df.columns if col not in cols_to_exclude]

    # --- Column Ordering ---
    final_ordered_cols = []
    # Use a case-insensitive check for fixed order columns
    df_cols_lower = {c.lower(): c for c in df.columns}
    for col in fixed_order:
        if col.lower() in df_cols_lower:
            original_case_col = df_cols_lower[col.lower()]
            final_ordered_cols.append(original_case_col)
    
    remaining_cols = sorted([col for col in data_cols if col not in final_ordered_cols])
    final_ordered_cols.extend(remaining_cols)

    final_df = df[final_ordered_cols]
    final_df['source_filepath_for_download'] = source_filepaths
    
    return final_df.values.tolist(), final_ordered_cols, table_name.replace('_', ' ').title()


# --- API Endpoints ---

@app.get("/", response_class=HTMLResponse)
async def read_root():
    """Serves the main landing page."""
    return HTML_TEMPLATE.render()

@app.get("/data/{report_type}", response_class=HTMLResponse)
async def get_report_data(report_type: str):
    """Fetches and displays data for the selected report type."""
    data, headers, table_name = get_clean_ordered_data(report_type.upper())
    return HTML_TEMPLATE.render(data=data, headers=headers, table_name=table_name)

@app.get("/download/{filepath:path}")
async def download_file(filepath: str):
    """Provides a file for download."""
    full_path = os.path.join(FILES_BASE_DIRECTORY, filepath)
    if not os.path.exists(full_path):
        raise HTTPException(status_code=404, detail="File not found.")
    return FileResponse(path=full_path, filename=os.path.basename(full_path))

# To run this app:
# 1. Save it as viewer_app_interactive.py
# 2. Install requirements: pip install fastapi "uvicorn[standard]" jinja2 pandas
# 3. Run in your terminal: uvicorn viewer_app_interactive:app --reload

========================================

--- Directory: UNKNOWN_REPORTS ---

Scan completed!
Total files processed: 9
Total files skipped: 5
